{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLSp__toe3QS"
      },
      "source": [
        "# **Text Summarization and Question Answering System for COVID-19 Data**\n",
        "\n",
        "Garima Chandna\n",
        "\n",
        "102017070"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# PART I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The Malaria Vaccine Implementation Programme in Africa: clarifying misperceptions\n",
            "\n",
            "\n",
            "Extensive testing in clinical trials has confirmed that the malaria vaccine, RTS,S/AS01, reduces malaria significantly, including life-threatening severe malaria. The Malaria Vaccine Implementation Programme (MVIP) is now underway to (i) support the pilot implementation of RTS,S/AS01 by routine health systems, and (ii) evaluate the routine use of the vaccine.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag # for proper noun\n",
        "from nltk.stem import PorterStemmer\n",
        "import math\n",
        "\n",
        "\n",
        "filename=\"2.txt\";\n",
        "lines=[ ]\n",
        "f = open((filename), \"r\");\n",
        "text=f.read() #append each line in the file to a list\n",
        "#print(text)\n",
        "f.close()\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(text)\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "word_tokens_lower=[word.lower() for word in word_tokens]\n",
        "stopWords = list(set(stopwords.words(\"english\")))\n",
        "word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n",
        "#print(len(word_tokens_refined))\n",
        "stem = [ ]\n",
        "ps = PorterStemmer( )\n",
        "for w in word_tokens_refined:\n",
        "    stem.append(ps.stem(w))\n",
        "word_tokens_refined=stem\n",
        "\n",
        "QPhrases=[\"coronavirus\",\"virus\", \"patient\",\"disease\", \"doctor\", \"covid-19\", \"who\", \"covid\", \"cases\",  \"vaccine\", \"cerf\", \"ppe\", \"healthcare\", \"government\", \"India\", \"therefore\"]\n",
        "cue_phrases={ }\n",
        "for sentence in sent_tokens:\n",
        "    cue_phrases[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for word in word_tokens:\n",
        "        if word.lower() in QPhrases:\n",
        "            cue_phrases[sentence] += 1\n",
        "maximum_frequency = max(cue_phrases.values())\n",
        "for k in cue_phrases.keys():\n",
        "    try:\n",
        "        cue_phrases[k] = (cue_phrases[k] / maximum_frequency)\n",
        "        cue_phrases[k]=round(cue_phrases[k],3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "#print(cue_phrases)\n",
        "\n",
        "\n",
        "\n",
        "numeric_data={ }\n",
        "for sentence in sent_tokens:\n",
        "    numeric_data[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for k in word_tokens:\n",
        "        if k.isdigit():\n",
        "            numeric_data[sentence] += 1\n",
        "maximum_frequency = max(numeric_data.values())\n",
        "for k in numeric_data.keys():\n",
        "    try:\n",
        "        numeric_data[k] = (numeric_data[k]/maximum_frequency)\n",
        "        numeric_data[k] = round(numeric_data[k], 3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "#print(numeric_data)\n",
        "\n",
        "sent_len_score={ }\n",
        "for sentence in sent_tokens:\n",
        "    sent_len_score[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    if len(word_tokens) in range(0,10):\n",
        "        sent_len_score[sentence]=1-0.05*(10-len(word_tokens))\n",
        "    elif len(word_tokens) in range(7,20):\n",
        "        sent_len_score[sentence]=1\n",
        "    else:\n",
        "        sent_len_score[sentence]=1-(0.05)*(len(word_tokens)-20)\n",
        "for k in sent_len_score.keys():\n",
        "    sent_len_score[k]=round(sent_len_score[k],4)\n",
        "#print(sent_len_score.values())\n",
        "\n",
        "\n",
        "\n",
        "sentence_position={ }\n",
        "d=1\n",
        "no_of_sent=len(sent_tokens)\n",
        "for i in range(no_of_sent):\n",
        "    a=1/d\n",
        "    b=1/(no_of_sent-d+1)\n",
        "    sentence_position[sent_tokens[d-1]]=max(a,b)\n",
        "    d=d+1\n",
        "for k in sentence_position.keys():\n",
        "    sentence_position[k]=round(sentence_position[k],3)\n",
        "#print(sentence_position.values())\n",
        "\n",
        "\n",
        "\n",
        "freqTable = { }\n",
        "for word in word_tokens_refined:\n",
        "    if word in freqTable:\n",
        "        freqTable[word] += 1\n",
        "    else:\n",
        "        freqTable[word] = 1\n",
        "for k in freqTable.keys():\n",
        "    freqTable[k]= math.log10(1+freqTable[k])\n",
        "\n",
        "word_frequency={ }\n",
        "for sentence in sent_tokens:\n",
        "    word_frequency[sentence]=0\n",
        "    e=nltk.word_tokenize(sentence)\n",
        "    f=[ ]\n",
        "    for word in e:\n",
        "        f.append(ps.stem(word))\n",
        "    for word,freq in freqTable.items():\n",
        "        if word in f:\n",
        "            word_frequency[sentence]+=freq\n",
        "#print(word_frequency.values())    \n",
        "\n",
        "\n",
        "upper_case={ }\n",
        "for sentence in sent_tokens:\n",
        "    upper_case[sentence] = 0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for k in word_tokens:\n",
        "        if k.isupper():\n",
        "            upper_case[sentence] += 1\n",
        "maximum_frequency = max(upper_case.values())\n",
        "for k in upper_case.keys():\n",
        "        try:\n",
        "            upper_case[k] = (upper_case[k]/maximum_frequency)\n",
        "            upper_case[k] = round(upper_case[k], 3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "#print(upper_case.values())\n",
        "\n",
        "\n",
        "\n",
        "proper_noun={ }\n",
        "for sentence in sent_tokens:\n",
        "    tagged_sent = pos_tag(sentence.split())\n",
        "    propernouns = [word for word, pos in tagged_sent if pos == 'NNP'] \n",
        "    proper_noun[sentence]=len(propernouns)\n",
        "maximum_frequency = max(proper_noun.values())\n",
        "for k in proper_noun.keys():\n",
        "    try:\n",
        "        proper_noun[k] = (proper_noun[k]/maximum_frequency)\n",
        "        proper_noun[k] = round(proper_noun[k], 3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "#print(proper_noun)\n",
        "\n",
        "\n",
        "head_match={ }\n",
        "heading=sent_tokens[0]\n",
        "for sentence in sent_tokens:\n",
        "    head_match[sentence]=0\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    for k in word_tokens:\n",
        "        if k not in stopWords:\n",
        "            k = ps.stem(k)\n",
        "            if k in ps.stem(heading):\n",
        "                head_match[sentence] += 1\n",
        "maximum_frequency = max(head_match.values())\n",
        "for k in head_match.keys():\n",
        "    try:\n",
        "        head_match[k] = (head_match[k]/maximum_frequency)\n",
        "        head_match[k] = round(head_match[k], 3)\n",
        "    except ZeroDivisionError:\n",
        "        x=0\n",
        "#print(head_match)\n",
        "\n",
        "\n",
        "total_score={}\n",
        "for k in cue_phrases.keys():\n",
        "    total_score[k]=cue_phrases[k]+numeric_data[k]+sent_len_score[k]+sentence_position[k]+word_frequency[k]+upper_case[k]+proper_noun[k]+head_match[k]\n",
        "#print(total_score.values()) \n",
        "\n",
        "sumValues = 0\n",
        "for sentence in total_score:\n",
        "    sumValues += total_score[sentence]\n",
        "average = int(sumValues / len(total_score))\n",
        "# Storing sentences into our summary.\n",
        "summary = ''\n",
        "for sentence in sent_tokens:\n",
        "    if (sentence in total_score) and (total_score[sentence] > (1.2*average)):\n",
        "        summary += \" \" + sentence\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PART II"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " This is a QA system. It will try to answer questions that start with Who, When or Where. Enter exit to leave the program.\n",
            "un releases us$15 million to help vulnerable countries battle the spread of the coronavirus\n",
            "\n",
            "Thank you! GoodBye\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#importing the required packages\n",
        "import nltk\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "\n",
        "            \n",
        "#creating the dummy variable flag to separate the response for exit\n",
        "flag=True\n",
        "\n",
        "\n",
        "#printing the welcome message onto console\n",
        "print(\" This is a QA system. It will try to answer questions that start with Who, When or Where. Enter exit to leave the program.\")\n",
        "while(flag==True):\n",
        "    #taking the input to user_response variable\n",
        "    user_response = input()\n",
        "    user_response_input=user_response\n",
        "    user_response=user_response.lower()\n",
        "    #if the user enters exit the program enters into this loop\n",
        "    if(user_response!='exit'):\n",
        "\n",
        "        sentences = []\n",
        "        with open('1.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                    line= line.lower()\n",
        "                    sentences.extend(line.split('.'))\n",
        "\n",
        "        max_prob = 0\n",
        "        similar_sentence = None\n",
        "        length = len(sentences)\n",
        "        for i in range(length):\n",
        "                match_ratio = SequenceMatcher(None, user_response,sentences[i]).ratio()\n",
        "                if  match_ratio > max_prob:\n",
        "                    max_prob = match_ratio\n",
        "                    similar_sentence = sentences[i]\n",
        "        if similar_sentence is not None:\n",
        "            print(similar_sentence)\n",
        "        else:\n",
        "            print('Sorry, don\\'t know the answer');    \n",
        "\n",
        "\n",
        "\n",
        "    else:\n",
        "        flag=False\n",
        "        #printing the thanks message to console when the user enters exit\n",
        "        print(\"Thank you! GoodBye\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "102017070_ELC.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ae98a2d7cb2d0c57cfbbd7d812947b84707417a4702cc81eb8890c9224ec2f85"
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
